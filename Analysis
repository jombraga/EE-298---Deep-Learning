For the CNN, different sizes (all powers of 2) of batchsize was tested. 128 was tested as it was used in class. It was tested on 20 epochs. The test accuracy was 73.5. Next, the epoch was doubled and yielded a test accuracy of 74.7%. Further increasing the epoch actually resulted in lower test accuracy. The network may be overfitting. Epoch was set at 40. Next, the batchsize was increased up to 512. As expected, the network suffered poor generalization, evident in lower test accuracy. The batchsize was then reduced up to 32. No improvement of test accuracy was observed relative to that of 128. Less than 128 is probably too small.

For the MLP, different optimizers were used such as SGD, AdaGrad but only Adam resulted to the higher accuracy. It was also observed that removing the dropout actually improved test accuracy from the order of 34.5 or so, to 51.9. The dropout may probably be hurting the capacity of the model.

With the test accuracies, CNN outperforms MLP on CIFAR-10 dataset.
