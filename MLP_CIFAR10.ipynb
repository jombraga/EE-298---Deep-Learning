{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP CIFAR10",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jombraga/EE-298---Deep-Learning/blob/master/MLP_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t8Ym2d1pNLy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5153ba0-a21d-4fe7-a641-98824a3886e1"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.utils import to_categorical, plot_model\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "# load mnist dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# compute the number of labels\n",
        "num_labels = len(np.unique(y_train))\n",
        "\n",
        "# convert to one-hot vector\n",
        "# e.g. 3 -> [0 0 0 1 0 0 0 0 0 0]\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# image dimensions (assumed square)\n",
        "image_size = x_train.shape[1]\n",
        "input_size = image_size * image_size * 3\n",
        "\n",
        "# resize and normalize\n",
        "x_train = np.reshape(x_train, [-1, input_size])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = np.reshape(x_test, [-1, input_size])\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# network parameters\n",
        "batch_size = 128\n",
        "hidden_units = 256\n",
        "\n",
        "\n",
        "\n",
        "# model is a 3-layer MLP with ReLU and dropout after each layer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "# plot_model(model, to_file='mlp-mnist.png', show_shapes=True)\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer with default lr=0.01\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "# train the network\n",
        "model.fit(x_train, y_train, epochs=40, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 256)               786688    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 855,050\n",
            "Trainable params: 855,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/40\n",
            "50000/50000 [==============================] - 8s 169us/step - loss: 1.8703 - acc: 0.3215\n",
            "Epoch 2/40\n",
            "50000/50000 [==============================] - 8s 161us/step - loss: 1.6718 - acc: 0.4028\n",
            "Epoch 3/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.5951 - acc: 0.4315\n",
            "Epoch 4/40\n",
            "50000/50000 [==============================] - 8s 165us/step - loss: 1.5389 - acc: 0.4523\n",
            "Epoch 5/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.5036 - acc: 0.4631\n",
            "Epoch 6/40\n",
            "50000/50000 [==============================] - 8s 162us/step - loss: 1.4577 - acc: 0.4803\n",
            "Epoch 7/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.4397 - acc: 0.4887\n",
            "Epoch 8/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.4151 - acc: 0.4964\n",
            "Epoch 9/40\n",
            "50000/50000 [==============================] - 8s 166us/step - loss: 1.3858 - acc: 0.5054\n",
            "Epoch 10/40\n",
            "50000/50000 [==============================] - 8s 162us/step - loss: 1.3678 - acc: 0.5121\n",
            "Epoch 11/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.3445 - acc: 0.5196\n",
            "Epoch 12/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.3342 - acc: 0.5248\n",
            "Epoch 13/40\n",
            "50000/50000 [==============================] - 8s 162us/step - loss: 1.3127 - acc: 0.5309\n",
            "Epoch 14/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.2959 - acc: 0.5396\n",
            "Epoch 15/40\n",
            "50000/50000 [==============================] - 8s 164us/step - loss: 1.2734 - acc: 0.5458\n",
            "Epoch 16/40\n",
            "50000/50000 [==============================] - 8s 161us/step - loss: 1.2685 - acc: 0.5478\n",
            "Epoch 17/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.2522 - acc: 0.5510\n",
            "Epoch 18/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.2338 - acc: 0.5582\n",
            "Epoch 19/40\n",
            "50000/50000 [==============================] - 8s 162us/step - loss: 1.2271 - acc: 0.5641\n",
            "Epoch 20/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.2083 - acc: 0.5693\n",
            "Epoch 21/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.2048 - acc: 0.5711\n",
            "Epoch 22/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.1828 - acc: 0.5787\n",
            "Epoch 23/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.1737 - acc: 0.5812\n",
            "Epoch 24/40\n",
            "50000/50000 [==============================] - 8s 163us/step - loss: 1.1757 - acc: 0.5814\n",
            "Epoch 25/40\n",
            "50000/50000 [==============================] - 8s 161us/step - loss: 1.1562 - acc: 0.5895\n",
            "Epoch 26/40\n",
            "50000/50000 [==============================] - 8s 162us/step - loss: 1.1482 - acc: 0.5919\n",
            "Epoch 27/40\n",
            "50000/50000 [==============================] - 8s 162us/step - loss: 1.1441 - acc: 0.5935\n",
            "Epoch 28/40\n",
            "50000/50000 [==============================] - 8s 164us/step - loss: 1.1342 - acc: 0.5951\n",
            "Epoch 29/40\n",
            "50000/50000 [==============================] - 8s 161us/step - loss: 1.1210 - acc: 0.6006\n",
            "Epoch 30/40\n",
            "50000/50000 [==============================] - 8s 160us/step - loss: 1.1118 - acc: 0.6052\n",
            "Epoch 31/40\n",
            "50000/50000 [==============================] - 8s 164us/step - loss: 1.1044 - acc: 0.6058\n",
            "Epoch 32/40\n",
            "50000/50000 [==============================] - 8s 165us/step - loss: 1.1037 - acc: 0.6059\n",
            "Epoch 33/40\n",
            "50000/50000 [==============================] - 8s 165us/step - loss: 1.0930 - acc: 0.6100\n",
            "Epoch 34/40\n",
            "50000/50000 [==============================] - 8s 164us/step - loss: 1.0825 - acc: 0.6146\n",
            "Epoch 35/40\n",
            "50000/50000 [==============================] - 8s 161us/step - loss: 1.0707 - acc: 0.6190\n",
            "Epoch 36/40\n",
            "50000/50000 [==============================] - 8s 162us/step - loss: 1.0657 - acc: 0.6210\n",
            "Epoch 37/40\n",
            "50000/50000 [==============================] - 8s 164us/step - loss: 1.0677 - acc: 0.6209\n",
            "Epoch 38/40\n",
            "50000/50000 [==============================] - 8s 165us/step - loss: 1.0559 - acc: 0.6246\n",
            "Epoch 39/40\n",
            "50000/50000 [==============================] - 8s 161us/step - loss: 1.0525 - acc: 0.6268\n",
            "Epoch 40/40\n",
            "50000/50000 [==============================] - 8s 162us/step - loss: 1.0393 - acc: 0.6284\n",
            "10000/10000 [==============================] - 0s 46us/step\n",
            "\n",
            "Test accuracy: 49.9%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}